

With large NN we can increase the performance with large amount of data 
activation fucntion from sigmoid to relu 

ðŸ”· What is a Feed Forward Neural Network (FNN)?
An FNN is a type of neural network where the data flows in one direction â€” from input to output â€” without cycles or loops. That means there are no backward connections or feedback.

good for regression or multi calssicfication data 
--Limitation :
not good for sequential or text data like time series data 

Parameters - w, b
HyperParameter - alpha , hideen layer(L), activation function , hidden units n1..., 

ðŸ§  What is Mini-Batch Gradient Descent?
Gradient Descent (GD) is an optimization algorithm used to minimize loss. It comes in three main types:
Type | Description | Pros | Cons
Batch GD | Uses entire dataset to compute gradient | Stable updates | Slow, memory-intensive
Stochastic GD (SGD) | Uses 1 sample at a time for gradient updates | Fast updates, low memory | Very noisy, can zigzag
Mini-Batch GD | Uses small batch (e.g. 32, 64) at a time | Best of both worlds | Needs tuning of batch size

ðŸš€ What is RMSProp?
It adapts the learning rate for each parameter individually, based on recent gradients â€” allowing faster and more stable convergence.
Smoothens the gradient using a moving average.


